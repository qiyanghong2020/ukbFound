{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "initial_id",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30965/4118936084.py:12: DtypeWarning: Columns (5,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  traits_df = pd.read_csv('ukb_traits.csv', encoding='latin1', quotechar='\"')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_index= 0\n",
      "chunk_index= 1\n",
      "chunk_index= 2\n",
      "Save data chunk_index= 0\n",
      "chunk_index= 3\n",
      "chunk_index= 4\n",
      "Save data chunk_index= 1\n",
      "Save data chunk_index= 2\n",
      "chunk_index= 5\n",
      "chunk_index= 6\n",
      "Save data chunk_index= 3\n",
      "Save data chunk_index= 4\n",
      "chunk_index= 7\n",
      "Save data chunk_index= 5\n",
      "chunk_index= 8\n",
      "chunk_index= 9\n",
      "Save data chunk_index= 6\n",
      "chunk_index= 10\n",
      "n_future= 0\n",
      "n_future= 1\n",
      "Save data chunk_index= 7\n",
      "n_future= 2\n",
      "n_future= 3\n",
      "n_future= 4\n",
      "n_future= 5\n",
      "n_future= 6\n",
      "n_future= 7\n",
      "Save data chunk_index= 8\n",
      "n_future= 8\n",
      "Save data chunk_index= 9\n",
      "n_future= 9\n",
      "Save data chunk_index= 10\n",
      "n_future= 10\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "# 读取traits_df\n",
    "traits_df = pd.read_csv('ukb_traits.csv', encoding='latin1', quotechar='\"')\n",
    "# 定义文件路径\n",
    "file_path = \"ukb43164.csv\"\n",
    "output_file = \"output_data.csv\"  # 输出文件\n",
    "outfile = Path('tmp_out.csv')\n",
    "\n",
    "\n",
    "# 自定义函数去掉浮点数中的 .0\n",
    "def convert_and_strip(value):\n",
    "    if pd.isna(value):  # 处理 NaN\n",
    "        return 'nan'\n",
    "    if isinstance(value, float) and value.is_integer():\n",
    "        return str(int(value))  # 浮点数如果是整数，则转换为整数\n",
    "    return str(value).strip()\n",
    "    \n",
    "traits_df = traits_df.map(convert_and_strip)\n",
    "\n",
    "def check_row(apply_row, df):\n",
    "    # 将所有数据转换为字符串\n",
    "    value_types = df['value_type'].unique()\n",
    "    field_ids = df['field_id'].unique()\n",
    "    field_id = field_ids[0]\n",
    "    \n",
    "    if not len(field_ids) == 1:\n",
    "        print(f\"field_ids have more than one value: {field_ids}\")\n",
    "        raise KeyError\n",
    "    \n",
    "    out_dict = OrderedDict()\n",
    "    value_type = value_types[0]\n",
    "    array_values = apply_row.to_list()\n",
    "    \n",
    "    if value_type == '22':  # 多选题\n",
    "        if field_id not in ['20002', '20001']:\n",
    "            # df 是trait df, 这样筛选出来, 21只会筛选1个, 22会多个,  多选题只保留回答的选项, 空值不保留. 相对位置保持和df中预定一致. 每个回答匹配的array就作为一个col\n",
    "            # 如果有匹配到值, 每个匹配的值就是输出文件的一个新的列, 如果没有匹配到值, 就是返回空表\n",
    "            matched_value_df = df[df['value'].isin(array_values)] \n",
    "            count = 0 \n",
    "            for index, row in matched_value_df.iterrows():\n",
    "                col = row['trait'] + '_d' + apply_row.index[0] + '-' + str(count)  # 每个匹配到值的array给一个统一的名, 方便不同值的array紧凑组合.\n",
    "                out_dict[col] = row['token_id']\n",
    "                count += 1\n",
    "        else:\n",
    "            if len(apply_row.unique()) > 1:  # 对于全部都是nan的选项就不考虑赋值了\n",
    "                # 对['20002', '20001'], 这两种记录所有的选项, 即便是没有匹配到值的选项也记录.\n",
    "                for index, row in df.iterrows():\n",
    "                    choice_value = row['value']\n",
    "                    col = row['trait'] + '_d' + apply_row.index[0]  + '-' + choice_value  # apply_row.index[0]]用来识别读取的是原始数据哪里\n",
    "                    selected_apply_row = apply_row[apply_row == choice_value]\n",
    "                    if selected_apply_row.empty:\n",
    "                        # 没匹配到预设的疾病, 那么这个疾病在这个患者种就是空的, 用not的token id\n",
    "                        out_dict[col] = df[(df['value'] == row['value']) & (df['token'].str.contains('_\"not\"'))]['token_id'].iat[0]\n",
    "                    else:\n",
    "                        out_dict[col] = df[(df['value'] == row['value']) & (df['token'].str.contains('_\"yes\"'))]['token_id'].iat[0]\n",
    "    else:  # value type 11, 31, 21\n",
    "        matched_value_df = df[df['value'].isin(array_values)] \n",
    "        col = df['trait'].iat[0] + '_d' + apply_row.index[0] # 这三种类型只能有一个column\n",
    "\n",
    "        if matched_value_df.empty:\n",
    "            # 没有匹配到任何预定值情况下, 应该就是连续值了\n",
    "            if 'Q1' in df['value'].values:\n",
    "                # 用mean统一所有连续值\n",
    "                value = float(apply_row.astype(float).mean())\n",
    "                \n",
    "                for q in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "                    matched_value_df = df[df['value'] == q]\n",
    "                    if value <= float(matched_value_df['meaning'].iat[0]):\n",
    "                        out_dict[col] = matched_value_df['token_id'].iat[0]\n",
    "                        break\n",
    "            else:\n",
    "                out_dict[col] = np.nan  # 这个患者没回答任何值, 本身是空值.\n",
    "        else:\n",
    "            out_dict[col] = matched_value_df['token_id'].iat[0]\n",
    "\n",
    "\n",
    "    out_s = pd.Series(out_dict)\n",
    "    return out_s\n",
    "\n",
    "def process_chunk(chunk_index, chunk_df, only_traits_df):\n",
    "    out_chunk_df = pd.DataFrame({'eid': chunk_df['eid']})\n",
    "    missing_field = []\n",
    "    for row_index, row in enumerate(only_traits_df.itertuples(), 1):\n",
    "        #if row_index>300:\n",
    "        #    break\n",
    "        instance_min = row.instance_min\n",
    "        field_id = row.field_id\n",
    "        try:\n",
    "            selected_columns_chunk_df = chunk_df[[f'{row.field_id}-{instance_min}.{array}' for array in range(int(row.array_min), int(row.array_max) + 1)]]\n",
    "        except KeyError:\n",
    "            missing_field.append(f'{row.field_id}-{instance_min}')\n",
    "            continue\n",
    "        if traits_df[traits_df['field_id'] == row.field_id]['meaning'].iat[0] in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "            continue\n",
    "        if not selected_columns_chunk_df.empty:\n",
    "            token_trait_df = selected_columns_chunk_df.apply(check_row, axis=1, args=(traits_df[traits_df['field_id'] == row.field_id],))\n",
    "            out_chunk_df = pd.concat([out_chunk_df, token_trait_df], axis=1, ignore_index=False)\n",
    "        else:\n",
    "            print(f\"No columns match for {row.field_id}-{instance_min} in chunk {chunk_index + 1}\")\n",
    "\n",
    "    \n",
    "    # 保存到CSV，第一个chunk使用写模式，之后使用追加模式\n",
    "    print('Save data chunk_index=', chunk_index)\n",
    "    out_chunk_df.to_csv(outfile, mode='a', header=True, index=False)        \n",
    "    return out_chunk_df, missing_field\n",
    "    \n",
    "\n",
    "def submit_with_resource_limit(executor, func, *args, max_workers, cpu_threshold=80, mem_threshold=80, sleep_interval=10, **kwargs):\n",
    "    while True:\n",
    "        if len(executor._pending_work_items) < max_workers:\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            mem_usage = psutil.virtual_memory().percent\n",
    "\n",
    "            if cpu_usage < cpu_threshold and mem_usage < mem_threshold:\n",
    "                return executor.submit(func, *args, **kwargs)\n",
    "            else:\n",
    "                print(f\"High resource usage detected: CPU {cpu_usage}%, Memory {mem_usage}%. Waiting for resources to be available...\")\n",
    "                time.sleep(sleep_interval)\n",
    "        else:\n",
    "            print(\"Max workers limit reached. Waiting for an available slot...\")\n",
    "            time.sleep(sleep_interval)\n",
    "\n",
    "# 读取数据并处理\n",
    "chunksize = 2\n",
    "only_traits_df = traits_df[['field_id', 'value_type', 'instance_min', 'array_min', 'array_max', 'private']].drop_duplicates()\n",
    "only_traits_df = only_traits_df[only_traits_df['private'] == '0']\n",
    "\n",
    "max_workers = 15  # 设置最大并行线程数\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    outfile.unlink(missing_ok=True)\n",
    "    futures = []\n",
    "    for chunk_index, chunk_df in enumerate(pd.read_csv(file_path, chunksize=chunksize, encoding='latin1', low_memory=False, quotechar='\"')):\n",
    "        #if chunk_index >10:\n",
    "        #    break\n",
    "        chunk_df = chunk_df.map(convert_and_strip)\n",
    "        print('chunk_index=', chunk_index)\n",
    "        #futures.append(submit_with_resource_limit(executor, process_chunk, chunk_index, chunk_df, only_traits_df))\n",
    "        future = submit_with_resource_limit(executor, process_chunk, chunk_index, chunk_df, only_traits_df, max_workers=max_workers)\n",
    "        futures.append(future)\n",
    "    \n",
    "    out_df = pd.DataFrame()\n",
    "    all_missing_fields = []\n",
    "\n",
    "    n_future = 0\n",
    "    for future in as_completed(futures):\n",
    "        out_chunk_df, missing_field = future.result()\n",
    "        print('n_future=', n_future)\n",
    "        n_future += 1\n",
    "        out_df = pd.concat([out_df, out_chunk_df], ignore_index=True)\n",
    "        all_missing_fields.extend(missing_field)\n",
    "    \n",
    "    out_df.to_csv(output_file, mode='w', header=True, index=False)\n",
    "    pd.DataFrame({'missing_field_id': all_missing_fields}).to_csv('missing_field_id.csv', mode='w', header=True, index=False)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
