{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0ef631-e20f-476e-9bf5-62658b144bf2",
   "metadata": {},
   "source": [
    "# UK Biobank Trait–Value Tokenisation Script\n",
    "\n",
    "This script converts UK Biobank raw phenotype fields into **token IDs** according to the unified trait–value vocabulary used in **ukbFound**.\n",
    "It is designed to reproduce the end-to-end preprocessing workflow for the demo dataset provided in `ukbFound/data/`.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "* Load the curated `ukb_traits.csv` vocabulary.\n",
    "* Map raw UKB fields (continuous, multi-choice, multi-select, categorical) into standardised token IDs.\n",
    "* Expand multi-instance and multi-array fields following UKB conventions.\n",
    "* Produce a tokenised participant-by-trait matrix suitable for downstream modelling.\n",
    "\n",
    "## Input files\n",
    "\n",
    "Located in `ukbFound/data/`:\n",
    "\n",
    "* `ukb_traits.csv` — unified trait–value vocabulary with field type, value levels and token IDs.\n",
    "* `demo_UKB_data.csv` — synthetic demo phenotype table (raw UKB-style columns).\n",
    "\n",
    "## Output files\n",
    "\n",
    "Generated in the same directory:\n",
    "\n",
    "* `demo_UKB_tokens.csv` — tokenised participant matrix (one row per participant).\n",
    "* `missing_field_id.csv` — list of (field_id, instance) not present in the input file.\n",
    "\n",
    "## Key processing steps\n",
    "\n",
    "1. **Load trait vocabulary** (`ukb_traits.csv`) as strings for type safety.\n",
    "2. **Select non-private traits** and enumerate their instance/array expansions.\n",
    "3. **Row-wise tokenisation** via `check_row()`\n",
    "\n",
    "   * Continuous: Q1–Q4 discretisation when applicable.\n",
    "   * Multi-choice: direct value lookup.\n",
    "   * Multi-select: “yes/not” encoding for 20001/20002; enumerated outputs for other fields.\n",
    "4. **Chunked streaming** over the raw CSV to support large datasets.\n",
    "5. **Concatenate outputs** and write to disk incrementally.\n",
    "\n",
    "## Notes\n",
    "\n",
    "* This implementation is self-contained and depends only on `pandas` and `numpy`.\n",
    "* All empty outputs are returned as `pd.Series(..., dtype=\"float64\")` to ensure stable downstream behaviour.\n",
    "* The demo dataset is small (n=100) and intended solely for illustrating the tokenisation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d5694f-0439-45a7-bf31-97b56b18a2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE    : /data/hongqy/dev/ukbFound/examples\n",
      "ROOT    : /data/hongqy/dev/ukbFound\n",
      "DATA_DIR: /data/hongqy/dev/ukbFound/data\n",
      "=== Step 1: Load trait vocabulary (ukb_traits.csv) ===\n",
      "Loaded traits_df with 44285 rows and 38 columns.\n",
      "Input UKB file   : /data/hongqy/dev/ukbFound/data/demo_UKB_data.csv\n",
      "Output token file: /data/hongqy/dev/ukbFound/data/demo_UKB_tokens.csv\n",
      "Missing-field log: /data/hongqy/dev/ukbFound/data/missing_field_id.csv\n",
      "Converting selected columns in traits_df to string representation ...\n",
      "\n",
      "=== Step 2: Summary of non-private traits used for tokenisation ===\n",
      "Unique non-private (field_id, instance_min, array_range) entries: 2257\n",
      "Value_type distribution among non-private traits:\n",
      "11     236\n",
      "21     859\n",
      "22      74\n",
      "31    1088\n",
      "Name: value_type, dtype: int64\n",
      "\n",
      "=== Step 3: Tokenise UKB CSV into ukbFound input tokens ===\n",
      "\n",
      "--- Processing chunk 1 ---\n",
      "  Rows in this chunk: 99\n",
      "  Columns in raw chunk: 18831\n",
      "  [chunk 1] processed traits 2257/2257\n",
      "  Finished traits for chunk 1.\n",
      "  Output columns in this chunk (including eid): 2667\n",
      "  Chunk 1 written to disk.\n",
      "\n",
      "=== Step 4: Final summary ===\n",
      "Total chunks processed : 1\n",
      "Total participants (rows) processed: 99\n",
      "Tokenised data written to: /data/hongqy/dev/ukbFound/data/demo_UKB_tokens.csv\n",
      "Number of (field_id-instance) not found in input CSV: 207\n",
      "Example missing entries (up to 10):\n",
      "  - 100001-0\n",
      "  - 100002-0\n",
      "  - 100003-0\n",
      "  - 100004-0\n",
      "  - 100005-0\n",
      "  - 100006-0\n",
      "  - 100007-0\n",
      "  - 100008-0\n",
      "  - 100009-0\n",
      "  - 100011-0\n",
      "Full list saved to: /data/hongqy/dev/ukbFound/data/missing_field_id.csv\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "# ========== 0. 路径设置 ==========\n",
    "# 当前脚本所在目录（例如 ukbFound/examples/）；在 Notebook 中则使用当前工作目录\n",
    "try:\n",
    "    HERE = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    HERE = Path.cwd()\n",
    "\n",
    "# 仓库根目录，例如 ukbFound/\n",
    "ROOT = HERE.parent\n",
    "# data 目录，例如 ukbFound/data/\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "print(\"HERE    :\", HERE)\n",
    "print(\"ROOT    :\", ROOT)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "\n",
    "# ========== 1. 加载 trait 词表 ==========\n",
    "print(\"=== Step 1: Load trait vocabulary (ukb_traits.csv) ===\")\n",
    "\n",
    "# 关键改动：dtype=str，避免混合类型 DtypeWarning\n",
    "traits_df = pd.read_csv(\n",
    "    DATA_DIR / \"ukb_traits.csv\",\n",
    "    encoding=\"latin1\",\n",
    "    quotechar='\"',\n",
    "    dtype=str,\n",
    ")\n",
    "\n",
    "print(f\"Loaded traits_df with {traits_df.shape[0]} rows and {traits_df.shape[1]} columns.\")\n",
    "\n",
    "file_path = DATA_DIR / \"demo_UKB_data.csv\"\n",
    "output_file = DATA_DIR / \"demo_UKB_tokens.csv\"\n",
    "missing_field_file = DATA_DIR / \"missing_field_id.csv\"\n",
    "print(f\"Input UKB file   : {file_path}\")\n",
    "print(f\"Output token file: {output_file}\")\n",
    "print(f\"Missing-field log: {missing_field_file}\")\n",
    "\n",
    "# ========== 1.1 工具函数：统一字符串表示 ==========\n",
    "def convert_and_strip(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    # dtype=str 已保证是字符串，这里保守兼容 float/int\n",
    "    if isinstance(value, float) and value.is_integer():\n",
    "        return str(int(value))\n",
    "    return str(value).strip()\n",
    "\n",
    "print(\"Converting selected columns in traits_df to string representation ...\")\n",
    "\n",
    "traits_df[\"field_id\"] = traits_df[\"field_id\"].map(convert_and_strip)\n",
    "traits_df[\"value_type\"] = traits_df[\"value_type\"].map(convert_and_strip)\n",
    "traits_df[\"value\"] = traits_df[\"value\"].map(convert_and_strip)\n",
    "if \"trait\" in traits_df.columns:\n",
    "    traits_df[\"trait\"] = traits_df[\"trait\"].map(convert_and_strip)\n",
    "if \"token\" in traits_df.columns:\n",
    "    traits_df[\"token\"] = traits_df[\"token\"].map(str)\n",
    "if \"meaning\" in traits_df.columns:\n",
    "    traits_df[\"meaning\"] = traits_df[\"meaning\"].map(str)\n",
    "\n",
    "only_traits_df = traits_df[\n",
    "    [\"field_id\", \"value_type\", \"instance_min\", \"array_min\", \"array_max\", \"private\"]\n",
    "].drop_duplicates()\n",
    "only_traits_df = only_traits_df[only_traits_df[\"private\"] == \"0\"]  # 注意此处 dtype=str\n",
    "\n",
    "print(\"\\n=== Step 2: Summary of non-private traits used for tokenisation ===\")\n",
    "print(f\"Unique non-private (field_id, instance_min, array_range) entries: {only_traits_df.shape[0]}\")\n",
    "print(\"Value_type distribution among non-private traits:\")\n",
    "print(only_traits_df[\"value_type\"].value_counts().sort_index())\n",
    "print()\n",
    "\n",
    "# 为了后面快速筛选，统一把 instance_min/array_min/array_max 转成 int\n",
    "only_traits_df = only_traits_df.copy()\n",
    "only_traits_df[\"instance_min\"] = only_traits_df[\"instance_min\"].astype(int)\n",
    "only_traits_df[\"array_min\"] = only_traits_df[\"array_min\"].astype(int)\n",
    "only_traits_df[\"array_max\"] = only_traits_df[\"array_max\"].astype(int)\n",
    "\n",
    "# ========== 2. per-row 处理逻辑 ==========\n",
    "def check_row(apply_row, df_trait):\n",
    "    \"\"\"\n",
    "    apply_row: 某个 field 在同一 instance 下的一整行（array 维度展开后的多个列）\n",
    "    df_trait:  当前 field_id 在 traits 词表中的子集\n",
    "    \"\"\"\n",
    "    apply_row = apply_row.map(convert_and_strip)\n",
    "    df = df_trait.copy()\n",
    "\n",
    "    value_types = df[\"value_type\"].unique()\n",
    "    field_ids = df[\"field_id\"].unique()\n",
    "    field_id = field_ids[0]\n",
    "\n",
    "    if len(field_ids) != 1:\n",
    "        raise KeyError(f\"field_ids have more than one value: {field_ids}\")\n",
    "\n",
    "    value_type = value_types[0]\n",
    "    array_values = [v for v in apply_row.tolist() if not pd.isna(v)]\n",
    "\n",
    "    out_dict = OrderedDict()\n",
    "\n",
    "    # ========== 2.1 多选题 ==========\n",
    "    if value_type == \"22\":\n",
    "        # 普通多选\n",
    "        if field_id not in [\"20002\", \"20001\"]:\n",
    "            if len(array_values) == 0:\n",
    "                # 关键改动：显式指定 dtype，避免 FutureWarning\n",
    "                return pd.Series(out_dict, dtype=\"float64\")\n",
    "\n",
    "            matched_value_df = df[df[\"value\"].isin(array_values)]\n",
    "            count = 0\n",
    "            for _, row in matched_value_df.iterrows():\n",
    "                col = (\n",
    "                    row[\"trait\"]\n",
    "                    + \"_d\"\n",
    "                    + apply_row.index[0]\n",
    "                    + \"-\"\n",
    "                    + str(count)\n",
    "                )\n",
    "                out_dict[col] = float(row[\"token_id\"]) if \"token_id\" in row and not pd.isna(row[\"token_id\"]) else np.nan\n",
    "                count += 1\n",
    "        # 20002/20001 特殊编码: YES / NOT\n",
    "        else:\n",
    "            if len(apply_row.unique()) <= 1 and pd.isna(apply_row.unique()[0]):\n",
    "                # 关键改动：显式指定 dtype\n",
    "                return pd.Series(out_dict, dtype=\"float64\")\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                choice_value = row[\"value\"]\n",
    "                col = (\n",
    "                    row[\"trait\"]\n",
    "                    + \"_d\"\n",
    "                    + apply_row.index[0]\n",
    "                    + \"-\"\n",
    "                    + choice_value\n",
    "                )\n",
    "\n",
    "                selected_apply_row = apply_row[apply_row == choice_value]\n",
    "                yes_mask = (df[\"value\"] == row[\"value\"]) & (\n",
    "                    df[\"token\"].str.contains('_\"yes\"')\n",
    "                )\n",
    "                not_mask = (df[\"value\"] == row[\"value\"]) & (\n",
    "                    df[\"token\"].str.contains('_\"not\"')\n",
    "                )\n",
    "\n",
    "                if selected_apply_row.empty:\n",
    "                    if not df[not_mask].empty:\n",
    "                        out_dict[col] = float(df[not_mask][\"token_id\"].iat[0])\n",
    "                    else:\n",
    "                        out_dict[col] = np.nan\n",
    "                else:\n",
    "                    if not df[yes_mask].empty:\n",
    "                        out_dict[col] = float(df[yes_mask][\"token_id\"].iat[0])\n",
    "                    else:\n",
    "                        out_dict[col] = np.nan\n",
    "\n",
    "    # ========== 2.2 其他类型（11, 21, 31） ==========\n",
    "    else:\n",
    "        col = df[\"trait\"].iat[0] + \"_d\" + apply_row.index[0]\n",
    "        if len(array_values) == 0:\n",
    "            out_dict[col] = np.nan\n",
    "        else:\n",
    "            matched_value_df = df[df[\"value\"].isin(array_values)]\n",
    "\n",
    "            if matched_value_df.empty:\n",
    "                # Q1–Q4 分位数编码\n",
    "                if \"Q1\" in df[\"value\"].values:\n",
    "                    try:\n",
    "                        numeric_vals = pd.to_numeric(apply_row.dropna(), errors=\"coerce\")\n",
    "                        numeric_vals = numeric_vals.dropna()\n",
    "                        if len(numeric_vals) == 0:\n",
    "                            out_dict[col] = np.nan\n",
    "                        else:\n",
    "                            value = float(numeric_vals.mean())\n",
    "                            for q in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "                                q_df = df[df[\"value\"] == q]\n",
    "                                if q_df.empty:\n",
    "                                    continue\n",
    "                                boundary = float(q_df[\"meaning\"].iat[0])\n",
    "                                if value <= boundary:\n",
    "                                    out_dict[col] = float(q_df[\"token_id\"].iat[0])\n",
    "                                    break\n",
    "                            else:\n",
    "                                q4_df = df[df[\"value\"] == \"Q4\"]\n",
    "                                out_dict[col] = (\n",
    "                                    float(q4_df[\"token_id\"].iat[0]) if not q4_df.empty else np.nan\n",
    "                                )\n",
    "                    except Exception:\n",
    "                        out_dict[col] = np.nan\n",
    "                else:\n",
    "                    out_dict[col] = np.nan\n",
    "            else:\n",
    "                out_dict[col] = float(matched_value_df[\"token_id\"].iat[0])\n",
    "\n",
    "    # 关键改动：统一显式 dtype，避免 FutureWarning\n",
    "    return pd.Series(out_dict, dtype=\"float64\")\n",
    "\n",
    "\n",
    "# ========== 3. 主循环：按 chunk 读取并写出 ==========\n",
    "print(\"=== Step 3: Tokenise UKB CSV into ukbFound input tokens ===\")\n",
    "\n",
    "missing_field = set()\n",
    "total_rows = 0\n",
    "n_chunks = 0\n",
    "\n",
    "reader = pd.read_csv(\n",
    "    file_path,\n",
    "    chunksize=100000,\n",
    "    encoding=\"latin1\",\n",
    "    low_memory=False,\n",
    "    quotechar='\"',\n",
    ")\n",
    "\n",
    "for chunk_index, chunk_df in enumerate(reader):\n",
    "    n_chunks += 1\n",
    "    n_rows_chunk = chunk_df.shape[0]\n",
    "    total_rows += n_rows_chunk\n",
    "\n",
    "    print(f\"\\n--- Processing chunk {chunk_index + 1} ---\")\n",
    "    print(f\"  Rows in this chunk: {n_rows_chunk}\")\n",
    "    print(f\"  Columns in raw chunk: {chunk_df.shape[1]}\")\n",
    "\n",
    "    out_chunk_df = pd.DataFrame({\"eid\": chunk_df[\"eid\"]})\n",
    "\n",
    "    for row_index, row in enumerate(only_traits_df.itertuples(), 1):\n",
    "        instance_min = row.instance_min\n",
    "        field_id = str(row.field_id)\n",
    "        value_type = row.value_type\n",
    "\n",
    "        col_names = [\n",
    "            f\"{field_id}-{instance_min}.{array_idx}\"\n",
    "            for array_idx in range(row.array_min, row.array_max + 1)\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            selected_columns_chunk_df = chunk_df[col_names]\n",
    "        except KeyError:\n",
    "            missing_field.add(f\"{field_id}-{instance_min}\")\n",
    "            continue\n",
    "\n",
    "        if selected_columns_chunk_df.empty:\n",
    "            continue\n",
    "\n",
    "        token_trait_df = selected_columns_chunk_df.apply(\n",
    "            check_row,\n",
    "            axis=1,\n",
    "            args=(traits_df[traits_df[\"field_id\"] == field_id],),\n",
    "        )\n",
    "        out_chunk_df = pd.concat(\n",
    "            [out_chunk_df, token_trait_df],\n",
    "            axis=1,\n",
    "            ignore_index=False,\n",
    "        )\n",
    "\n",
    "        if row_index % 100 == 0 or row_index == only_traits_df.shape[0]:\n",
    "            print(\n",
    "                f\"  [chunk {chunk_index + 1}] processed traits \"\n",
    "                f\"{row_index}/{only_traits_df.shape[0]}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "    print()\n",
    "    print(f\"  Finished traits for chunk {chunk_index + 1}.\")\n",
    "    print(f\"  Output columns in this chunk (including eid): {out_chunk_df.shape[1]}\")\n",
    "\n",
    "    if chunk_index == 0:\n",
    "        out_chunk_df.to_csv(output_file, mode=\"w\", header=True, index=False)\n",
    "    else:\n",
    "        out_chunk_df.to_csv(output_file, mode=\"a\", header=False, index=False)\n",
    "\n",
    "    print(f\"  Chunk {chunk_index + 1} written to disk.\")\n",
    "\n",
    "print(\"\\n=== Step 4: Final summary ===\")\n",
    "print(f\"Total chunks processed : {n_chunks}\")\n",
    "print(f\"Total participants (rows) processed: {total_rows}\")\n",
    "print(f\"Tokenised data written to: {output_file}\")\n",
    "\n",
    "if len(missing_field) > 0:\n",
    "    missing_sorted = sorted(missing_field)\n",
    "    pd.DataFrame({\"missing_field_id\": missing_sorted}).to_csv(\n",
    "        missing_field_file, mode=\"w\", header=True, index=False\n",
    "    )\n",
    "    print(f\"Number of (field_id-instance) not found in input CSV: {len(missing_sorted)}\")\n",
    "    print(\"Example missing entries (up to 10):\")\n",
    "    for m in missing_sorted[:10]:\n",
    "        print(f\"  - {m}\")\n",
    "    print(f\"Full list saved to: {missing_field_file}\")\n",
    "else:\n",
    "    print(\"All requested (field_id, instance_min) were found in the input CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc33ea-a76f-4186-b2c6-fbf2bdb2a6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ukb)",
   "language": "python",
   "name": "ukb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
